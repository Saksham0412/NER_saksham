# NER_saksham

This project demonstrates how to download, preprocess, and use the CoNLL-2003 dataset for Named Entity Recognition (NER) with BERT from Google Research.

## Overview

Named Entity Recognition (NER) is a fundamental task in Natural Language Processing (NLP) that involves classifying tokens in text as names of real-world entities, such as people, organizations, and locations. In this project, we use BERT, a powerful transformer-based model, to perform NER on the CoNLL-2003 dataset.

## Dataset

The CoNLL-2003 dataset is widely used for NER tasks and includes labeled entities in four categories:

- **PER**: Person
- **ORG**: Organization
- **LOC**: Location
- **MISC**: Miscellaneous

The dataset is provided in multiple files (e.g., `eng.train`, `eng.testa`, `eng.testb`), each containing sentences with tokens, POS tags, chunk tags, and entity labels.

### Dataset Download Link

The CoNLL-2003 dataset can be accessed and downloaded [here](https://paperswithcode.com/dataset/conll-2003).

## Setup and Requirements

### Prerequisites

- Python 3.6 or higher
- `transformers` library for BERT
- `pandas` for data manipulation
- `sklearn` for data splitting
- `requests` and `zipfile` for downloading and extracting data

### Installation

1. Clone this repository:
   ```bash
   git clone https://github.com/saksham0412/NER_saksham.git
## Steps to Run

1. **Download and Extract the Dataset**  
   The script will automatically download and extract the CoNLL-2003 dataset if itâ€™s not already available locally.

2. **Preprocess the Dataset**  
   The script parses the CoNLL-2003 dataset files, processes each sentence and its respective labels, and stores them in a DataFrame. Each sentence is tokenized, and labels are formatted for BERT input.

3. **Tokenize Sentences**  
   Using the BERT tokenizer, each sentence is converted into token IDs with padding and truncation to ensure uniform length. The tokenized data is saved for training and testing.

4. **Training and Evaluation**  
   The processed and tokenized data is ready for model training and evaluation using BERT.

## Code Structure

- **`download_and_preprocess.py`**: Script to download, extract, and preprocess the CoNLL-2003 dataset.
- **`bert_ner_model.py`**: Script to train a BERT model for NER using the preprocessed data.
- **`train_conll2003.csv`**: Processed training data (generated by the script).
- **`test_conll2003.csv`**: Processed test data (generated by the script).
